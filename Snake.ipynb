{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a00f07fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygame in c:\\users\\jorge\\anaconda3\\lib\\site-packages (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8782fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10908/2002237126.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m \u001b[0mgameLoop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10908/2002237126.py\u001b[0m in \u001b[0;36mgameLoop\u001b[1;34m()\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mgame_close\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mdis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[0mmessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You Lost! Press C-Play Again or Q-Quit\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mYour_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLength_of_snake\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import time\n",
    "import random\n",
    " \n",
    "pygame.init()\n",
    " \n",
    "white = (255, 255, 255)\n",
    "yellow = (255, 255, 102)\n",
    "black = (0, 0, 0)\n",
    "red = (213, 50, 80)\n",
    "green = (0, 255, 0)\n",
    "blue = (50, 153, 213)\n",
    " \n",
    "dis_width = 600\n",
    "dis_height = 400\n",
    " \n",
    "dis = pygame.display.set_mode((dis_width, dis_height))\n",
    "pygame.display.set_caption('Snake Game by Edureka')\n",
    " \n",
    "clock = pygame.time.Clock()\n",
    " \n",
    "snake_block = 10\n",
    "snake_speed = 15\n",
    " \n",
    "font_style = pygame.font.SysFont(\"bahnschrift\", 25)\n",
    "score_font = pygame.font.SysFont(\"comicsansms\", 35)\n",
    " \n",
    " \n",
    "def Your_score(score):\n",
    "    value = score_font.render(\"Your Score: \" + str(score), True, yellow)\n",
    "    dis.blit(value, [0, 0])\n",
    " \n",
    " \n",
    " \n",
    "def our_snake(snake_block, snake_list):\n",
    "    for x in snake_list:\n",
    "        pygame.draw.rect(dis, black, [x[0], x[1], snake_block, snake_block])\n",
    " \n",
    " \n",
    "def message(msg, color):\n",
    "    mesg = font_style.render(msg, True, color)\n",
    "    dis.blit(mesg, [dis_width / 6, dis_height / 3])\n",
    " \n",
    " \n",
    "def gameLoop():\n",
    "    game_over = False\n",
    "    game_close = False\n",
    " \n",
    "    x1 = dis_width / 2\n",
    "    y1 = dis_height / 2\n",
    " \n",
    "    x1_change = 0\n",
    "    y1_change = 0\n",
    " \n",
    "    snake_List = []\n",
    "    Length_of_snake = 1\n",
    " \n",
    "    foodx = round(random.randrange(0, dis_width - snake_block) / 10.0) * 10.0\n",
    "    foody = round(random.randrange(0, dis_height - snake_block) / 10.0) * 10.0\n",
    " \n",
    "    while not game_over:\n",
    " \n",
    "        while game_close == True:\n",
    "            dis.fill(blue)\n",
    "            message(\"You Lost! Press C-Play Again or Q-Quit\", red)\n",
    "            Your_score(Length_of_snake - 1)\n",
    "            pygame.display.update()\n",
    " \n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_q:\n",
    "                        game_over = True\n",
    "                        game_close = False\n",
    "                    if event.key == pygame.K_c:\n",
    "                        gameLoop()\n",
    " \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                game_over = True\n",
    "            if event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_LEFT:\n",
    "                    x1_change = -snake_block\n",
    "                    y1_change = 0\n",
    "                elif event.key == pygame.K_RIGHT:\n",
    "                    x1_change = snake_block\n",
    "                    y1_change = 0\n",
    "                elif event.key == pygame.K_UP:\n",
    "                    y1_change = -snake_block\n",
    "                    x1_change = 0\n",
    "                elif event.key == pygame.K_DOWN:\n",
    "                    y1_change = snake_block\n",
    "                    x1_change = 0\n",
    " \n",
    "        if x1 >= dis_width or x1 < 0 or y1 >= dis_height or y1 < 0:\n",
    "            game_close = True\n",
    "        x1 += x1_change\n",
    "        y1 += y1_change\n",
    "        dis.fill(blue)\n",
    "        pygame.draw.rect(dis, green, [foodx, foody, snake_block, snake_block])\n",
    "        snake_Head = []\n",
    "        snake_Head.append(x1)\n",
    "        snake_Head.append(y1)\n",
    "        snake_List.append(snake_Head)\n",
    "        if len(snake_List) > Length_of_snake:\n",
    "            del snake_List[0]\n",
    " \n",
    "        for x in snake_List[:-1]:\n",
    "            if x == snake_Head:\n",
    "                game_close = True\n",
    " \n",
    "        our_snake(snake_block, snake_List)\n",
    "        Your_score(Length_of_snake - 1)\n",
    " \n",
    "        pygame.display.update()\n",
    " \n",
    "        if x1 == foodx and y1 == foody:\n",
    "            foodx = round(random.randrange(0, dis_width - snake_block) / 10.0) * 10.0\n",
    "            foody = round(random.randrange(0, dis_height - snake_block) / 10.0) * 10.0\n",
    "            Length_of_snake += 1\n",
    " \n",
    "        clock.tick(snake_speed)\n",
    " \n",
    "    pygame.quit()\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c491193",
   "metadata": {},
   "source": [
    "# Creación de la Replay Memory\n",
    "Buffer circular en el que se almacenan resultados previos, que serán empleados en la fase learn del algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539fb8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self,number_of_observations):\n",
    "        # Create replay memory\n",
    "        self.states = np.zeros((MEMORY_SIZE, number_of_observations))\n",
    "        self.states_next = np.zeros((MEMORY_SIZE, number_of_observations))\n",
    "        self.actions = np.zeros(MEMORY_SIZE, dtype=np.int32)\n",
    "        self.rewards = np.zeros(MEMORY_SIZE)\n",
    "        self.terminal_states = np.zeros(MEMORY_SIZE, dtype=bool)\n",
    "        self.size=0\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_next, terminal_state):\n",
    "        # Store a transition (s,a,r,s') in the replay memory\n",
    "        i = self.size\n",
    "        self.states[i%MEMORY_SIZE] = state\n",
    "        self.states_next[i%MEMORY_SIZE] = state_next\n",
    "        self.actions[i%MEMORY_SIZE] = action\n",
    "        self.rewards[i%MEMORY_SIZE] = reward\n",
    "        self.terminal_states[i%MEMORY_SIZE] = terminal_state\n",
    "        self.size += 1\n",
    "\n",
    "    def sample_memory(self, batch_size):\n",
    "        # Generate a sample of transitions from the replay memory\n",
    "        batch = np.random.choice(min(MEMORY_SIZE, self.size), batch_size)\n",
    "        states = self.states[batch]\n",
    "        states_next = self.states_next[batch]\n",
    "        rewards = self.rewards[batch]\n",
    "        actions = self.actions[batch]   \n",
    "        terminal_states = self.terminal_states[batch]  \n",
    "        return states, actions, rewards, states_next, terminal_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceacc76b",
   "metadata": {},
   "source": [
    "# DQN\n",
    "Clase que maneja toda la parte de la Deep Q neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978ddcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "\n",
    "    def __init__(self, number_of_observations, number_of_actions):\n",
    "        # Initialize variables and create neural model\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.number_of_actions = number_of_actions\n",
    "        self.number_of_observations = number_of_observations\n",
    "        self.scores = []\n",
    "        self.avg_scores=[]\n",
    "        self.loss=[]\n",
    "        self.avg_loss=[]\n",
    "        self.memory = ReplayMemory(number_of_observations)\n",
    "        #Q network\n",
    "        self.model = keras.models.Sequential()\n",
    "\n",
    "        self.model.add(keras.layers.Dense(500, input_shape=(number_of_observations,), \\\n",
    "                             activation=\"relu\",kernel_initializer=\"he_normal\"))\n",
    "        self.model.add(keras.layers.Dense(250, activation=\"relu\",kernel_initializer=\"he_normal\"))\n",
    "        self.model.add(keras.layers.Dense(100, activation=\"relu\",kernel_initializer=\"he_normal\"))\n",
    "        self.model.add(keras.layers.Dense(number_of_actions, activation=\"linear\"))\n",
    "\n",
    "        self.model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE))\n",
    "        #target network\n",
    "        self.target_net = keras.models.Sequential()\n",
    "\n",
    "        self.target_net.add(keras.layers.Dense(500, input_shape=(number_of_observations,), \\\n",
    "                             activation=\"relu\",kernel_initializer=\"he_normal\"))\n",
    "        self.target_net.add(keras.layers.Dense(250, activation=\"relu\",kernel_initializer=\"he_normal\"))\n",
    "        self.target_net.add(keras.layers.Dense(100, activation=\"relu\",kernel_initializer=\"he_normal\"))\n",
    "        self.target_net.add(keras.layers.Dense(number_of_actions, activation=\"linear\"))\n",
    "\n",
    "        self.target_net.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE))\n",
    "        self.actualize_target()\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, terminal_state):\n",
    "        # Store a tuple (s, a, r, s') for experience replay\n",
    "        state = np.reshape(state, [1, self.number_of_observations])\n",
    "        next_state = np.reshape(next_state, [1, self.number_of_observations])\n",
    "        self.memory.store_transition(state, action, reward, next_state, terminal_state)\n",
    "\n",
    "    def select(self, state):\n",
    "        # Generate an action for a given state using epsilon-greedy policy\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.number_of_actions)\n",
    "        else:\n",
    "            state = np.reshape(state, [1, self.number_of_observations])\n",
    "            q_values = self.model.predict(state)\n",
    "            return np.argmax(q_values[0])\n",
    "\n",
    "    def learn(self):\n",
    "        # Learn the value Q using a sample of examples from the replay memory\n",
    "        if self.memory.size < BATCH_SIZE: return\n",
    "\n",
    "        states, actions, rewards, next_states, terminal_states = self.memory.sample_memory(BATCH_SIZE)\n",
    "\n",
    "        q_targets = self.model.predict(states)\n",
    "        q_next_states = self.target_net.predict(next_states)\n",
    "        for i in range(BATCH_SIZE):\n",
    "             if (terminal_states[i]):\n",
    "                  q_targets[i][actions[i]] = rewards[i]\n",
    "             else:\n",
    "                  q_targets[i][actions[i]] = rewards[i] + GAMMA * np.max(q_next_states[i])    \n",
    "\n",
    "        loss=self.model.train_on_batch(states, q_targets)\n",
    "        return loss\n",
    "        # Decrease exploration rate\n",
    "        #self.exploration_rate *= EXPLORATION_DECAY\n",
    "        #self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "    def decayExploration(self, score):\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "    def add_score(self, score, avg_scores, loss, avg_loss):\n",
    "       # Add the obtained score in a list to be presented later\n",
    "        self.scores.append(score)\n",
    "        self.avg_scores.append(avg_scores)\n",
    "        self.loss.append(loss)\n",
    "        self.avg_loss.append(avg_loss)\n",
    "\n",
    "    def display_scores_graphically(self):\n",
    "        # Display the obtained scores graphically\n",
    "        plt.plot(self.scores)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(self.avg_scores)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Avg Score\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(self.loss)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(self.avg_loss)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Avg Loss\")\n",
    "        plt.show()\n",
    "\n",
    "    def actualize_target(self):\n",
    "        for i in range(len(self.model.layers)):\n",
    "          weights=self.model.get_layer(index=i).get_weights()\n",
    "          layer=self.target_net.get_layer(index=i)\n",
    "          layer.set_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d661a",
   "metadata": {},
   "source": [
    "# Entorno del problema\n",
    "## Vector de estados:\n",
    "    state[0]=Snake x\n",
    "    state[1]=Snake y\n",
    "    state[2]=Food x\n",
    "    state[3]=Food y\n",
    "    \n",
    "## Acciones\n",
    "    0 ir arriba\n",
    "    1 ir derecha\n",
    "    2 ir abajo\n",
    "    3 ir izquierda\n",
    "\n",
    "## Recompensas\n",
    " ### Recompensas positivas:\n",
    "     +100 por comer la comida\n",
    " ### Recompensas negativas:\n",
    "     -Distancia a la comida, penalizar alejarse de la comida, para intentar crear el comportamiente de ir a por comida\n",
    "\n",
    "## Condiciones de finalización:\n",
    "    Chocarse con el borde\n",
    "    Chocarse con su propio cuerpo\n",
    "    Comer 50 comidas sin morir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
